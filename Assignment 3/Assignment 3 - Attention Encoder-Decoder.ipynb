{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fac9571d-1d18-45d4-b6d6-96b59889ed01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import stuff\n",
    "from io import open\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3533d5ff-0453-49c2-a3a4-4449057bd73a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8ddc3457c8f469287ef2239d19dd406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa7bd8549e4f431487127987173379db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bc579f1e5584006a162a9393761807d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from RuNNE import RuNNEBuilder\n",
    "builder = RuNNEBuilder()\n",
    "builder.download_and_prepare()\n",
    "dataset = builder.as_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45d7ebac-3a53-4325-aa19-8d9714c92a20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('ent_types.txt') as file:\n",
    "    classes = [line.rstrip() for line in file]\n",
    "class2id = {class_:id for id, class_ in enumerate(classes)}\n",
    "id2class = {id:class_ for class_, id in class2id.items()}\n",
    "\n",
    "classes = ['NONE'] + classes\n",
    "\n",
    "NUM_CLASSES = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e5f2ee2-9da7-460b-8261-6e8ed34af718",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('blinoff/roberta-base-russian-v0', max_len=512)\n",
    "\n",
    "NUM_TOKENS = len(tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22ed3303-7f60-4c33-92b4-b17fd3bbd1fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = dataset['train']\n",
    "test = dataset['test']\n",
    "dev = dataset['dev']\n",
    "\n",
    "MAX_LENGTH = 512\n",
    "MAX_LABELS = 128\n",
    "\n",
    "def pad_or_truncate(some_list, target_len, default=0):\n",
    "    return some_list[:target_len] + [default]*(target_len - len(some_list))\n",
    "\n",
    "def preprocess_function(example):\n",
    "    text = example['text']\n",
    "    ids = tokenizer(text)['input_ids']\n",
    "    all_labels = example['entities']\n",
    "    split_labels = list(map(lambda x: x.split(), all_labels))\n",
    "    labels = []\n",
    "    \n",
    "    for label in split_labels:\n",
    "        start = int(label[0]) / len(text)\n",
    "        end = int(label[1]) / len(text)\n",
    "        id = class2id[label[2]]\n",
    "        labels.append((start, end, id))\n",
    "    labels = sorted(labels)\n",
    "    \n",
    "    ids = pad_or_truncate(ids, MAX_LENGTH)\n",
    "    labels = pad_or_truncate(labels, MAX_LABELS, (-1, -1, 0))\n",
    "    \n",
    "    bounds = []\n",
    "    _classes = []\n",
    "    #seq_len = len(ids)\n",
    "    for label in labels:\n",
    "        bounds.append((label[0], label[1]))\n",
    "        _classes.append(label[2])\n",
    "        \n",
    "    return {'tokens': ids, 'bounds': bounds, 'classes': _classes}\n",
    "    #return {'tokens': ids, 'labels': labels}\n",
    "    \n",
    "def onehot(example):\n",
    "    return {'classes': F.one_hot(example['classes'], NUM_CLASSES)}\n",
    "    \n",
    "    \n",
    "    \n",
    "train_processed = train.map(preprocess_function, batched=False)\n",
    "test_processed = test.map(preprocess_function, batched=False)\n",
    "#dev_processed = dev.map(preprocess_function, batched=True)\n",
    "\n",
    "train_processed = train_processed.with_format(\"torch\")\n",
    "test_processed = test_processed.with_format(\"torch\")\n",
    "    \n",
    "#train_processed = train_processed.map(onehot, batched=False)\n",
    "#test_processed = test_processed.map(onehot, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68aab248-68fe-40dd-877a-bf5affd7631a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataloader(dataset, batch_size=1):\n",
    "    \n",
    "    tokens = dataset['tokens']\n",
    "    bounds = dataset['bounds']\n",
    "    _classes = dataset['classes']\n",
    "    \n",
    "    train_data = TensorDataset(torch.LongTensor(tokens).to(device),\n",
    "                               torch.FloatTensor(bounds).to(device),\n",
    "                               torch.LongTensor(_classes).to(device),)\n",
    "    \n",
    "    dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9b1eba9-ece3-4efd-aa22-dd753274ca2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, hidden = self.gru(embedded)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9012cc7f-453e-4abb-8882-2f152eab14fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Va = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.bmm(weights, keys)\n",
    "\n",
    "        return context, weights\n",
    "\n",
    "class ClassifierDecoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
    "        super(ClassifierDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.attention = BahdanauAttention(hidden_size)\n",
    "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(0)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "        attentions = []\n",
    "\n",
    "        for i in range(MAX_LABELS):\n",
    "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attentions.append(attn_weights)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        attentions = torch.cat(attentions, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "\n",
    "        return decoder_outputs, decoder_hidden, attentions\n",
    "\n",
    "\n",
    "    def forward_step(self, input, hidden, encoder_outputs):\n",
    "        #print(input.shape)\n",
    "        embedded =  self.dropout(self.embedding(input))\n",
    "        #print(embedded.shape)\n",
    "\n",
    "        query = hidden.permute(1, 0, 2)\n",
    "        context, attn_weights = self.attention(query, encoder_outputs)\n",
    "        input_gru = torch.cat((embedded, context), dim=2)\n",
    "\n",
    "        output, hidden = self.gru(input_gru, hidden)\n",
    "        output = self.out(output)\n",
    "\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da689948-48a4-4d20-b358-5ee37612217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionDecoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
    "        super(RegressionDecoder, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.embedding = nn.Linear(output_size, hidden_size)\n",
    "        self.attention = BahdanauAttention(hidden_size)\n",
    "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, self.output_size, dtype=torch.float, device=device).fill_(0)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "        attentions = []\n",
    "\n",
    "        for i in range(MAX_LABELS):\n",
    "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attentions.append(attn_weights)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                decoder_input = decoder_output.detach()  # detach from history as input\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        attentions = torch.cat(attentions, dim=1)\n",
    "\n",
    "        return decoder_outputs, decoder_hidden, attentions\n",
    "\n",
    "\n",
    "    def forward_step(self, input, hidden, encoder_outputs):\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        if embedded.dim() == 2:\n",
    "            embedded = embedded.unsqueeze(1)\n",
    "            \n",
    "        #print(embedded.shape)\n",
    "        query = hidden.permute(1, 0, 2)\n",
    "        context, attn_weights = self.attention(query, encoder_outputs)\n",
    "        #print(embedded.shape, context.shape)        \n",
    "        input_gru = torch.cat((embedded, context), dim=2)\n",
    "\n",
    "        output, hidden = self.gru(input_gru, hidden)\n",
    "        output = self.out(output)\n",
    "        output = F.relu(output)\n",
    "\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c14a5b66-6e7a-4459-a052-208e220745b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, encoder, decoder1, decoder2, encoder_optimizer,\n",
    "          decoder1_optimizer, decoder2_optimizer, criterion1, criterion2):\n",
    "\n",
    "    total_loss1 = 0\n",
    "    total_loss2 = 0\n",
    "    for data in dataloader:\n",
    "        input_tensor, target_bounds, target_classes = data\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder1_optimizer.zero_grad()\n",
    "        decoder2_optimizer.zero_grad()\n",
    "        \n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs1, _, _ = decoder1(encoder_outputs, encoder_hidden, target_bounds)\n",
    "        decoder_outputs2, _, _ = decoder2(encoder_outputs, encoder_hidden, target_classes)\n",
    "        \n",
    "        #print(decoder_outputs1.view(-1).shape, target_bounds.view(-1).shape)\n",
    "        #print(decoder_outputs2.shape, target_classes.shape)\n",
    "\n",
    "        loss1 = criterion1(\n",
    "            decoder_outputs1.view(-1),\n",
    "            target_bounds.view(-1)\n",
    "        )\n",
    "        loss1.backward(retain_graph=True)\n",
    "\n",
    "        loss2 = criterion2(\n",
    "            decoder_outputs2.view(-1, decoder_outputs2.size(-1)),\n",
    "            target_classes.view(-1)\n",
    "        )\n",
    "        loss2.backward()\n",
    "        \n",
    "        encoder_optimizer.step()\n",
    "        decoder1_optimizer.step()\n",
    "        decoder2_optimizer.step()\n",
    "\n",
    "        total_loss1 += loss1.item()\n",
    "        total_loss2 += loss2.item()\n",
    "\n",
    "    return total_loss1 / len(dataloader), total_loss2 / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3445ad7d-fbd0-4697-8819-679fe60ae9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange\n",
    "\n",
    "def train(train_dataloader, encoder, decoder1, decoder2, n_epochs, learning_rate=0.001):\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder1_optimizer = optim.Adam(decoder1.parameters(), lr=learning_rate)\n",
    "    decoder2_optimizer = optim.Adam(decoder2.parameters(), lr=learning_rate)\n",
    "    criterion1 = nn.L1Loss()\n",
    "    criterion2 = nn.NLLLoss()\n",
    "    \n",
    "    bar = trange(1, n_epochs + 1)\n",
    "\n",
    "    for epoch in bar:\n",
    "        loss1, loss2 = train_epoch(train_dataloader, encoder, decoder1, decoder2, encoder_optimizer, decoder1_optimizer, decoder2_optimizer, criterion1, criterion2)\n",
    "        bar.set_description(f\"loss1={loss1} loss2={loss2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68455e79-9341-4c17-800d-4479efa2fc7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 24\n",
    "\n",
    "train_loader = get_dataloader(train_processed, batch_size)\n",
    "test_loader = get_dataloader(test_processed, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84f9a1c6-af88-40ec-a5bb-fc81bb4cd192",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5948bb8ed6814a17a48564fe184c4772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "\n",
    "encoder = EncoderRNN(NUM_TOKENS, hidden_size).to(device)\n",
    "decoder1 = RegressionDecoder(hidden_size, 2).to(device)\n",
    "decoder2 = ClassifierDecoder(hidden_size, NUM_CLASSES).to(device)\n",
    "\n",
    "train(train_loader, encoder, decoder1, decoder2, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b07a3a-aa38-4df3-984b-04127df62d81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(encoder.state_dict(), 'dualnorm3/encoder.ckpt')\n",
    "torch.save(decoder1.state_dict(), 'dualnorm3/decoder1.ckpt')\n",
    "torch.save(decoder2.state_dict(), 'dualnorm3/decoder2.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7ca731e-60e7-40e7-922e-2d087d9e682b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "encoder = EncoderRNN(NUM_TOKENS, hidden_size).to(device)\n",
    "decoder1 = RegressionDecoder(hidden_size, 2).to(device)\n",
    "decoder2 = ClassifierDecoder(hidden_size, NUM_CLASSES).to(device)\n",
    "\n",
    "encoder.load_state_dict(torch.load('dualnorm3/encoder.ckpt'))\n",
    "decoder1.load_state_dict(torch.load('dualnorm3/decoder1.ckpt'))\n",
    "decoder2.load_state_dict(torch.load('dualnorm3/decoder2.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07b94006-47e8-43a3-be90-e9969a0aaf44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder1, decoder2, sentence, tokenizer):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = torch.LongTensor(tokenizer(sentence)['input_ids']).view(1, -1).to(device)\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder1_outputs, _, _ = decoder1(encoder_outputs, encoder_hidden)\n",
    "        decoder2_outputs, _, _ = decoder2(encoder_outputs, encoder_hidden)\n",
    "        \n",
    "        _, topi = decoder2_outputs.topk(1)\n",
    "        decoded_ids = topi.squeeze()\n",
    "        \n",
    "        decoder1_outputs = decoder1_outputs * len(sentence)\n",
    "        decoder1_outputs = decoder1_outputs.squeeze().round().type(torch.int64)\n",
    "\n",
    "        decoded_classes = []\n",
    "        for idx in decoded_ids:\n",
    "            if idx.item() == 0:\n",
    "                break\n",
    "            decoded_classes.append(id2class[idx.item()])\n",
    "            \n",
    "        decoded_bounds = []\n",
    "        for i in range(len(decoded_classes)):\n",
    "            decoded_bounds.append((decoder1_outputs[i][0].item(), decoder1_outputs[i][1].item()))\n",
    "            \n",
    "        strings = []\n",
    "        for i in range(len(decoded_classes)):\n",
    "            strings.append([decoded_bounds[i][0], decoded_bounds[i][1], decoded_classes[i]])\n",
    "    return strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54ea8f4c-b82c-4ef1-bb3d-7b6cbfd1eac0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Владелец «Бирмингема» получил шесть лет тюрьмы\\nмини|слева|«Сент-Эндрюс» — домашний стадион футбольного клуба «Бирмингем Сити»\\nВ пятницу, 7 марта суд Гонконга приговорил владельца футбольного клуба «Бирмингем Сити» Карсона Ёнга (Carson Yeung, также в некоторых источниках — Карсон Юнг; Карсон Ён) к шести годам тюремного заключения за мошенничество.\\n\\n54-летний бизнесмен был признан виновным в отмывании 55 миллионов фунтов стерлингов через его банковские счета в период с 2001 по 2007 годы.\\n\\nКарсон Ёнг стал владельцем «Бирмингема» в 2009 году, приобретя его за 81,5 миллионов фунтов стерлингов.\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test']['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab0a0e77-5944-4513-9c1c-8562339e43dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6060d86ad014936832f4ea108d1ddd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import trange\n",
    "test = dataset['dev']\n",
    "MAX_LENGTH = 512\n",
    "MAX_LABELS = 128\n",
    "results = []\n",
    "for i in trange(len(test)):\n",
    "    sentence = test['text'][i]\n",
    "    id = test['id'][i]\n",
    "    res = evaluate(encoder, decoder1, decoder2, sentence, tokenizer)\n",
    "    results.append({'id': id, 'sentences': sentence, 'ners': res})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d50d1863-e4ee-4326-902e-36d3e742d1ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('test.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for res in results:\n",
    "        f.write(json.JSONEncoder().encode(res)+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
