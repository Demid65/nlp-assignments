{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63e656f4-5965-4eb5-8368-9a40aca8d02b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ким Чен Нама убили с помощью запрещённого химо...</td>\n",
       "      <td>[0 12 PERSON, 56 72 ORGANIZATION, 64 72 COUNTR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Смена портретов на долларах\\nГарриет Табмен\\nК...</td>\n",
       "      <td>[19 27 MONEY, 28 42 PERSON, 52 67 MONEY, 68 77...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Новым генсеком ООН станет португалец Гутерреш\\...</td>\n",
       "      <td>[6 14 PROFESSION, 15 18 ORGANIZATION, 26 36 NA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Вахту принял\\n\\nУоррен Баффет назвал своего по...</td>\n",
       "      <td>[14 27 PERSON, 199 209 DATE, 232 260 PROFESSIO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>В Японии скончался старейший житель Земли — Дз...</td>\n",
       "      <td>[2 8 COUNTRY, 79 96 DATE, 116 122 COUNTRY, 134...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>Электрик руководит социал-демократами Дюссельд...</td>\n",
       "      <td>[0 8 PROFESSION, 68 72 ORGANIZATION, 73 101 TI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>Мужчина женился на тёще\\n\\nРумынский мужчина у...</td>\n",
       "      <td>[25 34 NATIONALITY, 129 138 PERSON, 154 161 CI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>Названы лауреаты премии «World Press Photo»\\nФ...</td>\n",
       "      <td>[61 67 COUNTRY, 68 78 PERSON, 258 286 ORGANIZA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>Первое поражение Серены Уильямс в финале Austr...</td>\n",
       "      <td>[0 6 ORDINAL, 17 31 PERSON, 41 56 EVENT, 57 71...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>Аделина Сотникова стала первой\\nАделина Сотник...</td>\n",
       "      <td>[0 17 PERSON, 24 30 ORDINAL, 31 48 PERSON, 49 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>461 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    Ким Чен Нама убили с помощью запрещённого химо...   \n",
       "1    Смена портретов на долларах\\nГарриет Табмен\\nК...   \n",
       "2    Новым генсеком ООН станет португалец Гутерреш\\...   \n",
       "3    Вахту принял\\n\\nУоррен Баффет назвал своего по...   \n",
       "4    В Японии скончался старейший житель Земли — Дз...   \n",
       "..                                                 ...   \n",
       "456  Электрик руководит социал-демократами Дюссельд...   \n",
       "457  Мужчина женился на тёще\\n\\nРумынский мужчина у...   \n",
       "458  Названы лауреаты премии «World Press Photo»\\nФ...   \n",
       "459  Первое поражение Серены Уильямс в финале Austr...   \n",
       "460  Аделина Сотникова стала первой\\nАделина Сотник...   \n",
       "\n",
       "                                              entities  \n",
       "0    [0 12 PERSON, 56 72 ORGANIZATION, 64 72 COUNTR...  \n",
       "1    [19 27 MONEY, 28 42 PERSON, 52 67 MONEY, 68 77...  \n",
       "2    [6 14 PROFESSION, 15 18 ORGANIZATION, 26 36 NA...  \n",
       "3    [14 27 PERSON, 199 209 DATE, 232 260 PROFESSIO...  \n",
       "4    [2 8 COUNTRY, 79 96 DATE, 116 122 COUNTRY, 134...  \n",
       "..                                                 ...  \n",
       "456  [0 8 PROFESSION, 68 72 ORGANIZATION, 73 101 TI...  \n",
       "457  [25 34 NATIONALITY, 129 138 PERSON, 154 161 CI...  \n",
       "458  [61 67 COUNTRY, 68 78 PERSON, 258 286 ORGANIZA...  \n",
       "459  [0 6 ORDINAL, 17 31 PERSON, 41 56 EVENT, 57 71...  \n",
       "460  [0 17 PERSON, 24 30 ORDINAL, 31 48 PERSON, 49 ...  \n",
       "\n",
       "[461 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from RuNNE import RuNNEBuilder\n",
    "\n",
    "builder = RuNNEBuilder()\n",
    "builder.download_and_prepare()\n",
    "dataset = builder.as_dataset()\n",
    "df = dataset['train'].to_pandas().drop(columns='id')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "993c0a9b-fbfe-4a88-a12e-0dad56b3c4fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the model\n",
    "from transformers import T5TokenizerFast, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = T5TokenizerFast.from_pretrained(\"UrukHan/t5-russian-summarization\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"UrukHan/t5-russian-summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76fb9dbf-00fc-4ca3-97e7-0505804f1911",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = dataset['train']\n",
    "test = dataset['test']\n",
    "dev = dataset['dev']\n",
    "\n",
    "# preprocess the dataset\n",
    "max_input_length = 256\n",
    "max_target_length = 256\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples['text']\n",
    "    _targets = examples['entities']\n",
    "    targets = []\n",
    "    for t in _targets:\n",
    "        targets.append(','.join(t))\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True, padding=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    #model_inputs.drop\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "479dc599-428d-44de-95ef-3c9b9b009054",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_processed = train.map(preprocess_function, batched=True)\n",
    "test_processed = test.map(preprocess_function, batched=True)\n",
    "#dev_processed = dev.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e382b7e-48df-4c9b-a262-9fe2bd166130",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'text', 'entities', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 461\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7f66510-8ae9-478f-bbb3-df64ece24f98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-28 12:40:15.920100: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-04-28 12:40:15.920146: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-04-28 12:40:16.775576: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-04-28 12:40:16.775717: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-04-28 12:40:16.775731: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/tmp/ipykernel_322133/3544475062.py:27: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"sacrebleu\") # using the metric from the example\n",
      "/home/demid/.local/lib/python3.10/site-packages/datasets/load.py:759: FutureWarning: The repository for sacrebleu contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/sacrebleu/sacrebleu.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "batch_size = 4\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    \"RuNNE-Training\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=50,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True, # set to True if you have CUDA, False if CUDA is not available\n",
    "    generation_max_length=128,\n",
    "    \n",
    ")\n",
    "\n",
    "# set up the data collator to pad the inputs and labels\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# set up the metrics for the training process.\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"sacrebleu\") # using the metric from the example\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result\n",
    "\n",
    "# set up the trainer itself\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_processed,\n",
    "    eval_dataset=test_processed,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d16c336a-7222-457d-b79a-807456d23476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5800' max='5800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5800/5800 2:07:06, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.663738</td>\n",
       "      <td>0.138500</td>\n",
       "      <td>81.215100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.466263</td>\n",
       "      <td>0.547500</td>\n",
       "      <td>123.107500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.393990</td>\n",
       "      <td>0.555200</td>\n",
       "      <td>124.354800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.338489</td>\n",
       "      <td>0.448400</td>\n",
       "      <td>121.268800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.743700</td>\n",
       "      <td>1.298689</td>\n",
       "      <td>0.664100</td>\n",
       "      <td>123.451600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.743700</td>\n",
       "      <td>1.267874</td>\n",
       "      <td>0.882500</td>\n",
       "      <td>124.914000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.743700</td>\n",
       "      <td>1.241440</td>\n",
       "      <td>0.778500</td>\n",
       "      <td>123.602200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.743700</td>\n",
       "      <td>1.223199</td>\n",
       "      <td>0.675900</td>\n",
       "      <td>121.806500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.293700</td>\n",
       "      <td>1.205301</td>\n",
       "      <td>0.970400</td>\n",
       "      <td>121.172000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.293700</td>\n",
       "      <td>1.196971</td>\n",
       "      <td>0.801600</td>\n",
       "      <td>121.096800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.293700</td>\n",
       "      <td>1.189170</td>\n",
       "      <td>0.646600</td>\n",
       "      <td>122.043000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.293700</td>\n",
       "      <td>1.180628</td>\n",
       "      <td>0.908500</td>\n",
       "      <td>121.462400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.211900</td>\n",
       "      <td>1.173040</td>\n",
       "      <td>0.900500</td>\n",
       "      <td>120.623700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.211900</td>\n",
       "      <td>1.169068</td>\n",
       "      <td>0.757800</td>\n",
       "      <td>120.817200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.211900</td>\n",
       "      <td>1.163895</td>\n",
       "      <td>0.770800</td>\n",
       "      <td>121.419400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.211900</td>\n",
       "      <td>1.155146</td>\n",
       "      <td>0.795300</td>\n",
       "      <td>120.806500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.211900</td>\n",
       "      <td>1.156762</td>\n",
       "      <td>0.787400</td>\n",
       "      <td>121.537600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.166800</td>\n",
       "      <td>1.151476</td>\n",
       "      <td>0.810300</td>\n",
       "      <td>120.860200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.166800</td>\n",
       "      <td>1.148648</td>\n",
       "      <td>0.742700</td>\n",
       "      <td>121.548400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.166800</td>\n",
       "      <td>1.145634</td>\n",
       "      <td>0.887200</td>\n",
       "      <td>120.967700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.166800</td>\n",
       "      <td>1.144600</td>\n",
       "      <td>1.006700</td>\n",
       "      <td>120.838700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.130600</td>\n",
       "      <td>1.145300</td>\n",
       "      <td>0.780100</td>\n",
       "      <td>121.311800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.130600</td>\n",
       "      <td>1.141164</td>\n",
       "      <td>1.145000</td>\n",
       "      <td>121.172000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.130600</td>\n",
       "      <td>1.137932</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>121.440900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.130600</td>\n",
       "      <td>1.139781</td>\n",
       "      <td>0.957700</td>\n",
       "      <td>120.978500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.107200</td>\n",
       "      <td>1.135477</td>\n",
       "      <td>1.054600</td>\n",
       "      <td>121.236600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.107200</td>\n",
       "      <td>1.134093</td>\n",
       "      <td>0.942900</td>\n",
       "      <td>120.548400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.107200</td>\n",
       "      <td>1.133718</td>\n",
       "      <td>0.838800</td>\n",
       "      <td>121.053800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.107200</td>\n",
       "      <td>1.138373</td>\n",
       "      <td>0.877200</td>\n",
       "      <td>121.473100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.107200</td>\n",
       "      <td>1.134401</td>\n",
       "      <td>0.946900</td>\n",
       "      <td>121.311800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.087400</td>\n",
       "      <td>1.136760</td>\n",
       "      <td>1.036200</td>\n",
       "      <td>121.301100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.087400</td>\n",
       "      <td>1.136922</td>\n",
       "      <td>1.012700</td>\n",
       "      <td>120.989200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.087400</td>\n",
       "      <td>1.136956</td>\n",
       "      <td>1.146400</td>\n",
       "      <td>121.462400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.087400</td>\n",
       "      <td>1.138862</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>121.279600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.066200</td>\n",
       "      <td>1.137301</td>\n",
       "      <td>1.169300</td>\n",
       "      <td>121.279600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.066200</td>\n",
       "      <td>1.136628</td>\n",
       "      <td>1.063300</td>\n",
       "      <td>121.247300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.066200</td>\n",
       "      <td>1.135881</td>\n",
       "      <td>1.218700</td>\n",
       "      <td>121.096800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.066200</td>\n",
       "      <td>1.136793</td>\n",
       "      <td>1.233200</td>\n",
       "      <td>121.247300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.058600</td>\n",
       "      <td>1.135003</td>\n",
       "      <td>1.325500</td>\n",
       "      <td>121.440900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.058600</td>\n",
       "      <td>1.135908</td>\n",
       "      <td>1.214400</td>\n",
       "      <td>121.193500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.058600</td>\n",
       "      <td>1.136477</td>\n",
       "      <td>1.023500</td>\n",
       "      <td>121.161300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.058600</td>\n",
       "      <td>1.136119</td>\n",
       "      <td>1.256300</td>\n",
       "      <td>121.150500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.058600</td>\n",
       "      <td>1.136330</td>\n",
       "      <td>1.348100</td>\n",
       "      <td>121.290300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.045600</td>\n",
       "      <td>1.136248</td>\n",
       "      <td>1.290500</td>\n",
       "      <td>121.419400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.045600</td>\n",
       "      <td>1.137640</td>\n",
       "      <td>1.297300</td>\n",
       "      <td>121.215100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.045600</td>\n",
       "      <td>1.137602</td>\n",
       "      <td>1.492200</td>\n",
       "      <td>121.096800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.045600</td>\n",
       "      <td>1.137445</td>\n",
       "      <td>1.167400</td>\n",
       "      <td>121.376300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.039700</td>\n",
       "      <td>1.137640</td>\n",
       "      <td>1.311100</td>\n",
       "      <td>121.172000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.039700</td>\n",
       "      <td>1.138439</td>\n",
       "      <td>1.365100</td>\n",
       "      <td>121.215100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.039700</td>\n",
       "      <td>1.138431</td>\n",
       "      <td>1.290400</td>\n",
       "      <td>121.193500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5800, training_loss=1.170102354904701, metrics={'train_runtime': 7627.0328, 'train_samples_per_second': 3.022, 'train_steps_per_second': 0.76, 'total_flos': 7018239688704000.0, 'train_loss': 1.170102354904701, 'epoch': 50.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do the thing. That would take some time.\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a689f3d-1c97-4291-8b2a-91000e881892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ceb8ce4-06b0-4ccc-9234-609d7ad4e364",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64}\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('t5rs-RuNNE-128')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d29d7d44-845f-485f-8f5d-40f6b30beda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_trained = T5TokenizerFast.from_pretrained(\"t5rs-RuNNE-128\")\n",
    "model_trained = AutoModelForSeq2SeqLM.from_pretrained(\"t5rs-RuNNE-128\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c91871b4-b551-4245-a247-a80427931c94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "910632f0d3a94fe9b1ee2ec6fba4ac9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>result</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FakTyrA анонсировал сингл «Психопат» и назначи...</td>\n",
       "      <td>0 8 ORGANIATION,17 31 ORGANIATION,40 50 ORGANI...</td>\n",
       "      <td>[0 7 PERSON, 27 35 WORK_OF_ART, 90 98 DATE, 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Умер создатель первого в мире индексного фонда...</td>\n",
       "      <td>107 113 PERSON,114 137 DATE,140 164 ORGANIATIO...</td>\n",
       "      <td>[15 22 ORDINAL, 47 56 PERSON, 127 145 ORGANIZA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Художник Александр Шилов отмечает юбилей\\n29 о...</td>\n",
       "      <td>0 9 PERSON,35 50 PROFESSION,55 67 PERSON,68 76...</td>\n",
       "      <td>[0 8 PROFESSION, 9 24 PERSON, 41 61 DATE, 79 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Полноценная встреча Трампа с Путиным не состои...</td>\n",
       "      <td>0 5 PROFESSION,15 23 PERSON,45 54 PERSON,61 73...</td>\n",
       "      <td>[20 26 PERSON, 29 36 PERSON, 56 63 PERSON, 64 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>День памяти и скорби начался в Брянске с тради...</td>\n",
       "      <td>2 8 CIT,27 38 FACILIT,48 56 FACILIT,68 78 FACI...</td>\n",
       "      <td>[0 20 EVENT, 31 38 CITY, 127 138 EVENT, 153 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Умер Эдуард Лимонов\\nЭдуард Лимонов\\nВо вторни...</td>\n",
       "      <td>0 12 PERSON,27 39 PROFESSION,50 61 PERSON,71 8...</td>\n",
       "      <td>[35 65 DATE, 72 86 PERSON, 97 103 AGE, 122 129...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Полицейские во Франции убили страсбургского ст...</td>\n",
       "      <td>0 8 NATIONALIT,16 23 ORGANIATION,39 49 CIT,60 ...</td>\n",
       "      <td>[15 22 COUNTRY, 29 43 CITY, 100 107 COUNTRY, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Литва празднует 100-летие независимости\\n\\nГер...</td>\n",
       "      <td>0 9 COUNTR,15 23 DATE,35 46 EVENT,58 67 COUNTR...</td>\n",
       "      <td>[0 5 COUNTRY, 16 25 AGE, 46 51 COUNTRY, 52 83 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>В элитном доме на Никитском бульваре полностью...</td>\n",
       "      <td>0 6 FACILIT,27 42 FACILIT,51 67 FACILIT,70 86 ...</td>\n",
       "      <td>[18 36 FACILITY, 71 83 PERSON, 84 101 FACILITY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Владимирский вице-губернатор покинул пост из-з...</td>\n",
       "      <td>0 8 PROFESSION,10 22 PERSON,52 61 PERSON,65 73...</td>\n",
       "      <td>[0 12 STATE_OR_PROVINCE, 92 111 PERSON, 162 17...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0  FakTyrA анонсировал сингл «Психопат» и назначи...   \n",
       "1  Умер создатель первого в мире индексного фонда...   \n",
       "2  Художник Александр Шилов отмечает юбилей\\n29 о...   \n",
       "3  Полноценная встреча Трампа с Путиным не состои...   \n",
       "4  День памяти и скорби начался в Брянске с тради...   \n",
       "5  Умер Эдуард Лимонов\\nЭдуард Лимонов\\nВо вторни...   \n",
       "6  Полицейские во Франции убили страсбургского ст...   \n",
       "7  Литва празднует 100-летие независимости\\n\\nГер...   \n",
       "8  В элитном доме на Никитском бульваре полностью...   \n",
       "9  Владимирский вице-губернатор покинул пост из-з...   \n",
       "\n",
       "                                              result  \\\n",
       "0  0 8 ORGANIATION,17 31 ORGANIATION,40 50 ORGANI...   \n",
       "1  107 113 PERSON,114 137 DATE,140 164 ORGANIATIO...   \n",
       "2  0 9 PERSON,35 50 PROFESSION,55 67 PERSON,68 76...   \n",
       "3  0 5 PROFESSION,15 23 PERSON,45 54 PERSON,61 73...   \n",
       "4  2 8 CIT,27 38 FACILIT,48 56 FACILIT,68 78 FACI...   \n",
       "5  0 12 PERSON,27 39 PROFESSION,50 61 PERSON,71 8...   \n",
       "6  0 8 NATIONALIT,16 23 ORGANIATION,39 49 CIT,60 ...   \n",
       "7  0 9 COUNTR,15 23 DATE,35 46 EVENT,58 67 COUNTR...   \n",
       "8  0 6 FACILIT,27 42 FACILIT,51 67 FACILIT,70 86 ...   \n",
       "9  0 8 PROFESSION,10 22 PERSON,52 61 PERSON,65 73...   \n",
       "\n",
       "                                              target  \n",
       "0  [0 7 PERSON, 27 35 WORK_OF_ART, 90 98 DATE, 10...  \n",
       "1  [15 22 ORDINAL, 47 56 PERSON, 127 145 ORGANIZA...  \n",
       "2  [0 8 PROFESSION, 9 24 PERSON, 41 61 DATE, 79 9...  \n",
       "3  [20 26 PERSON, 29 36 PERSON, 56 63 PERSON, 64 ...  \n",
       "4  [0 20 EVENT, 31 38 CITY, 127 138 EVENT, 153 16...  \n",
       "5  [35 65 DATE, 72 86 PERSON, 97 103 AGE, 122 129...  \n",
       "6  [15 22 COUNTRY, 29 43 CITY, 100 107 COUNTRY, 1...  \n",
       "7  [0 5 COUNTRY, 16 25 AGE, 46 51 COUNTRY, 52 83 ...  \n",
       "8  [18 36 FACILITY, 71 83 PERSON, 84 101 FACILITY...  \n",
       "9  [0 12 STATE_OR_PROVINCE, 92 111 PERSON, 162 17...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test if it is working on a part of test dataset\n",
    "from tqdm.notebook import trange\n",
    "input = []\n",
    "result = [] \n",
    "target = []\n",
    "N = 10\n",
    "\n",
    "for i in trange(N):\n",
    "    input_ids = tokenizer_trained(test['text'][i], return_tensors=\"pt\").input_ids\n",
    "    outputs = model_trained.generate(input_ids, max_new_tokens = 128)\n",
    "    input.append(test['text'][i])\n",
    "    result.append(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "    target.append(test['entities'][i])\n",
    "\n",
    "pd.DataFrame.from_dict({'input': input, 'result': result, 'target': target}).head(N)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
